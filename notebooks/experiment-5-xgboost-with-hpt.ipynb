{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8812b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install mlflow boto3 awscli optuna xgboost imbalanced-learn\n",
    "#! pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b67f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "# step 1 : Set up the mlflow tracking server \n",
    "mlflow.set_tracking_uri(\"http://ec2-13-62-127-174.eu-north-1.compute.amazonaws.com:5000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d0ae7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 21:51:27 INFO mlflow.tracking.fluent: Experiment with name 'Exp 5 - ML Algos with HP Tuning' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://reddit-reccomender-bucket/11', creation_time=1763396487435, experiment_id='11', last_update_time=1763396487435, lifecycle_stage='active', name='Exp 5 - ML Algos with HP Tuning', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set or create an experiment\n",
    "mlflow.set_experiment(\"Exp 5 - ML Algos with HP Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c04be81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report,f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ee5a4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36662, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('reddit_preprocessing.csv').dropna(subset=['clean_comment'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "730d8ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-18 00:28:49,556] A new study created in memory with name: no-name-3fcbde0a-eef2-4580-9c40-031b61961a09\n",
      "[I 2025-11-18 00:28:51,084] Trial 0 finished with value: 0.7440771380049607 and parameters: {'n_estimators': 130, 'learning_rate': 0.08992686140335425, 'max_depth': 7}. Best is trial 0 with value: 0.7440771380049607.\n",
      "[I 2025-11-18 00:28:52,629] Trial 1 finished with value: 0.6114513876364698 and parameters: {'n_estimators': 95, 'learning_rate': 0.008840014805722529, 'max_depth': 7}. Best is trial 0 with value: 0.7440771380049607.\n",
      "[I 2025-11-18 00:28:53,264] Trial 2 finished with value: 0.47641553065298137 and parameters: {'n_estimators': 98, 'learning_rate': 0.00010015873986564972, 'max_depth': 4}. Best is trial 0 with value: 0.7440771380049607.\n",
      "[I 2025-11-18 00:28:54,038] Trial 3 finished with value: 0.6759131879712167 and parameters: {'n_estimators': 69, 'learning_rate': 0.06217254759579635, 'max_depth': 6}. Best is trial 0 with value: 0.7440771380049607.\n",
      "[I 2025-11-18 00:28:59,919] Trial 4 finished with value: 0.6504452700202188 and parameters: {'n_estimators': 202, 'learning_rate': 0.008072452714252824, 'max_depth': 9}. Best is trial 0 with value: 0.7440771380049607.\n",
      "[I 2025-11-18 00:29:04,214] Trial 5 finished with value: 0.662021374259956 and parameters: {'n_estimators': 167, 'learning_rate': 0.010822826183571409, 'max_depth': 10}. Best is trial 0 with value: 0.7440771380049607.\n",
      "[I 2025-11-18 00:29:04,650] Trial 6 finished with value: 0.6423289545091055 and parameters: {'n_estimators': 102, 'learning_rate': 0.0329411389627439, 'max_depth': 4}. Best is trial 0 with value: 0.7440771380049607.\n",
      "[I 2025-11-18 00:29:07,785] Trial 7 finished with value: 0.565653704903913 and parameters: {'n_estimators': 240, 'learning_rate': 0.00027081129335413995, 'max_depth': 8}. Best is trial 0 with value: 0.7440771380049607.\n",
      "[I 2025-11-18 00:29:09,082] Trial 8 finished with value: 0.4769680787991375 and parameters: {'n_estimators': 294, 'learning_rate': 0.00020281844776347832, 'max_depth': 4}. Best is trial 0 with value: 0.7440771380049607.\n",
      "[I 2025-11-18 00:29:11,738] Trial 9 finished with value: 0.6053700521339294 and parameters: {'n_estimators': 112, 'learning_rate': 0.0012998738938752267, 'max_depth': 10}. Best is trial 0 with value: 0.7440771380049607.\n",
      "[I 2025-11-18 00:29:14,292] Trial 10 finished with value: 0.54398268060938 and parameters: {'n_estimators': 163, 'learning_rate': 0.0014216599771228928, 'max_depth': 6}. Best is trial 0 with value: 0.7440771380049607.\n",
      "[I 2025-11-18 00:29:15,059] Trial 11 finished with value: 0.6756764254158895 and parameters: {'n_estimators': 58, 'learning_rate': 0.07506113870472912, 'max_depth': 6}. Best is trial 0 with value: 0.7440771380049607.\n",
      "[I 2025-11-18 00:29:15,907] Trial 12 finished with value: 0.6888354244631797 and parameters: {'n_estimators': 52, 'learning_rate': 0.08523455908825647, 'max_depth': 7}. Best is trial 0 with value: 0.7440771380049607.\n",
      "[I 2025-11-18 00:29:18,117] Trial 13 finished with value: 0.6695142258896001 and parameters: {'n_estimators': 142, 'learning_rate': 0.022734212617096964, 'max_depth': 7}. Best is trial 0 with value: 0.7440771380049607.\n",
      "[I 2025-11-18 00:29:20,373] Trial 14 finished with value: 0.7522191033245815 and parameters: {'n_estimators': 139, 'learning_rate': 0.08885513575945109, 'max_depth': 8}. Best is trial 14 with value: 0.7522191033245815.\n",
      "[I 2025-11-18 00:29:22,743] Trial 15 finished with value: 0.6799220338109362 and parameters: {'n_estimators': 136, 'learning_rate': 0.025460745766139107, 'max_depth': 8}. Best is trial 14 with value: 0.7522191033245815.\n",
      "[I 2025-11-18 00:29:26,779] Trial 16 finished with value: 0.6122807801549489 and parameters: {'n_estimators': 198, 'learning_rate': 0.0034121816282804895, 'max_depth': 8}. Best is trial 14 with value: 0.7522191033245815.\n",
      "[I 2025-11-18 00:29:28,439] Trial 17 finished with value: 0.7018288593840883 and parameters: {'n_estimators': 206, 'learning_rate': 0.04117989155164772, 'max_depth': 5}. Best is trial 14 with value: 0.7522191033245815.\n",
      "[I 2025-11-18 00:29:32,185] Trial 18 finished with value: 0.6707112099723889 and parameters: {'n_estimators': 137, 'learning_rate': 0.01826086308903834, 'max_depth': 9}. Best is trial 14 with value: 0.7522191033245815.\n",
      "[I 2025-11-18 00:29:34,399] Trial 19 finished with value: 0.7552601398608829 and parameters: {'n_estimators': 124, 'learning_rate': 0.09891808585156875, 'max_depth': 9}. Best is trial 19 with value: 0.7552601398608829.\n",
      "[I 2025-11-18 00:29:39,835] Trial 20 finished with value: 0.6370326808640739 and parameters: {'n_estimators': 252, 'learning_rate': 0.004466811482000707, 'max_depth': 9}. Best is trial 19 with value: 0.7552601398608829.\n",
      "[I 2025-11-18 00:29:41,668] Trial 21 finished with value: 0.7526571251119238 and parameters: {'n_estimators': 123, 'learning_rate': 0.0997452424267896, 'max_depth': 8}. Best is trial 19 with value: 0.7552601398608829.\n",
      "[I 2025-11-18 00:29:43,377] Trial 22 finished with value: 0.6904776097779282 and parameters: {'n_estimators': 77, 'learning_rate': 0.04838295700925696, 'max_depth': 9}. Best is trial 19 with value: 0.7552601398608829.\n",
      "[I 2025-11-18 00:29:45,372] Trial 23 finished with value: 0.757210351493531 and parameters: {'n_estimators': 154, 'learning_rate': 0.09467535116586871, 'max_depth': 8}. Best is trial 23 with value: 0.757210351493531.\n",
      "[I 2025-11-18 00:29:50,180] Trial 24 finished with value: 0.6780899148277433 and parameters: {'n_estimators': 175, 'learning_rate': 0.014919565393082361, 'max_depth': 10}. Best is trial 23 with value: 0.757210351493531.\n",
      "[I 2025-11-18 00:29:52,069] Trial 25 finished with value: 0.6926835220230142 and parameters: {'n_estimators': 110, 'learning_rate': 0.040208513367189114, 'max_depth': 8}. Best is trial 23 with value: 0.757210351493531.\n",
      "[I 2025-11-18 00:29:52,909] Trial 26 finished with value: 0.4747791668515004 and parameters: {'n_estimators': 179, 'learning_rate': 0.001565016036748562, 'max_depth': 3}. Best is trial 23 with value: 0.757210351493531.\n",
      "[I 2025-11-18 00:29:55,713] Trial 27 finished with value: 0.7263793291644224 and parameters: {'n_estimators': 154, 'learning_rate': 0.04349166742179802, 'max_depth': 9}. Best is trial 23 with value: 0.757210351493531.\n",
      "[I 2025-11-18 00:29:57,736] Trial 28 finished with value: 0.7175128937570017 and parameters: {'n_estimators': 120, 'learning_rate': 0.05399109573233534, 'max_depth': 8}. Best is trial 23 with value: 0.757210351493531.\n",
      "[I 2025-11-18 00:29:59,986] Trial 29 finished with value: 0.7629403357085882 and parameters: {'n_estimators': 186, 'learning_rate': 0.09895440769820892, 'max_depth': 7}. Best is trial 29 with value: 0.7629403357085882.\n",
      "2025/11/18 00:30:11 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/11/18 00:30:27 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run XGBoost_Undersampling_BOW_Trigram at: http://ec2-51-21-223-34.eu-north-1.compute.amazonaws.com:5000/#/experiments/11/runs/84eccf773a75443e88e4d80d656a3681\n",
      "üß™ View experiment at: http://ec2-51-21-223-34.eu-north-1.compute.amazonaws.com:5000/#/experiments/11\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Remap the class labels from [-1, 0, 1] to [2, 0, 1] as xgboost doesnt read -1\n",
    "df['category'] = df.category.map({-1:2,0:0,1:1})\n",
    "\n",
    "# step 2:Remove rows where target label asre null\n",
    "df = df.dropna(subset=['category'])\n",
    "\n",
    "ngram_range = (1,3) #trigram from previous exp\n",
    "max_features = 1000 # max_feature from previous experiment\n",
    "\n",
    "# step 3: train test split before vectorization and resampling\n",
    "X_train,X_test,y_train,y_test = train_test_split(df['clean_comment'],df['category'],test_size=0.2,random_state=42,stratify=df['category'])\n",
    "\n",
    "#step 4: vectorization using BOW , fit on training data only\n",
    "vectorizer = CountVectorizer(ngram_range=ngram_range,max_features=max_features)\n",
    "X_train_vec= vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# use undersampler method as it came out best in exp4\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_vec,y_train = rus.fit_resample(X_train_vec,y_train)\n",
    "\n",
    "# Ensure integer labels\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "#Function to log results in MLflow\n",
    "def log_mlflow(model_name,model,X_train,X_test,y_train,y_test):\n",
    "    with mlflow.start_run():\n",
    "        #log the model type\n",
    "        mlflow.set_tag(\"mlflow.runName\", f\"{model_name}_Undersampling_BOW_Trigram\")\n",
    "        mlflow.set_tag(\"experiment_type\", \"Algorithm comparision\")\n",
    "\n",
    "        #log algorith name as parameter\n",
    "        mlflow.log_param(\"algo_name\",model_name)\n",
    "\n",
    "        #Train model\n",
    "        model.fit(X_train,y_train)\n",
    "        y_pred =model.predict(X_test)\n",
    "\n",
    "        # log accuracy\n",
    "        accuracy = accuracy_score(y_test,y_pred)\n",
    "        mlflow.log_metric(\"accuracy\",accuracy)\n",
    "        f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        mlflow.log_metric(\"f1_macro\",f1_macro)\n",
    "        f1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        mlflow.log_metric(\"f1_weighted\",f1_weighted)\n",
    "\n",
    "\n",
    "        # Log classification report\n",
    "        classification_rep = classification_report(y_test, y_pred, output_dict=True)\n",
    "        for label, metrics in classification_rep.items():\n",
    "            if isinstance(metrics, dict):\n",
    "                for metric, value in metrics.items():\n",
    "                    if metric != \"support\":\n",
    "                        mlflow.log_metric(f\"{label}_{metric}\", value)\n",
    "\n",
    "        #Log the model\n",
    "        mlflow.sklearn.log_model(model, f\"{model_name}_model\")\n",
    "\n",
    "#step 6: Optuna objective function for XG boost\n",
    "def objective_xgboost(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators',50,300)\n",
    "    learning_rate = trial.suggest_float('learning_rate',1e-4, 1e-1, log=True)\n",
    "    max_depth = trial.suggest_int('max_depth',3,10)\n",
    "\n",
    "    model = XGBClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate=learning_rate,\n",
    "    max_depth=max_depth,\n",
    "    random_state=42,\n",
    "    eval_metric=\"logloss\",\n",
    "    objective=\"multi:softmax\",   # FIX\n",
    "    num_class=3                  # FIX\n",
    ")\n",
    "    \n",
    "    preds = model.fit(X_train_vec, y_train).predict(X_test_vec)\n",
    "    return f1_score(y_test, preds, average='weighted')\n",
    "\n",
    "#stage 7 :run otuna for XG boost , log in the best model only \n",
    "def run_optuna_experiment():\n",
    "    study = optuna.create_study(direction= \"maximize\")\n",
    "    study.optimize(objective_xgboost,n_trials =30)\n",
    "\n",
    "    #get the best parameter and log only the best parameter\n",
    "    best_params = study.best_params\n",
    "    best_model = XGBClassifier(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    random_state=42,\n",
    "    eval_metric=\"logloss\",\n",
    "    objective=\"multi:softmax\",\n",
    "    num_class=3\n",
    ")\n",
    "\n",
    "        \n",
    "    # Log the best model with MLflow, passing the algo_name as \"xgboost\"\n",
    "    log_mlflow(\"XGBoost\", best_model, X_train_vec, X_test_vec, y_train, y_test)\n",
    "\n",
    "#run the experiment for XG boost\n",
    "run_optuna_experiment()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486d6713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_env (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
